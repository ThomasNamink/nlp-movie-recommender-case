{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
       "cell_type": "markdown",
       "metadata": {
          "id": "view-in-github",
           "colab_type": "text"
        },
        "source": [
          "<a href=\"https://colab.research.google.com/github/thomasnamink/nlp-movie-recommender-case/blob/main/NLP%20Case%202024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
       },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Recommender\n",
        "Welcome to the Movie Recommender case!\n",
        "\n",
        "---\n",
        "*Make sure to **make a copy** of this notebook and to work from there (File --> Save a Copy in Drive).*\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "83JVv_zAWBWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Getting started!\n",
        "Let's first install a required package"
      ],
      "metadata": {
        "id": "eoyFE-QdQk1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install a required package. Note that this may take a minute.\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "qMORftdzzJoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import pandas for data processing and set some configurations"
      ],
      "metadata": {
        "id": "tnRjEMYJUEkI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "orKUX1DGsLp8"
      },
      "outputs": [],
      "source": [
        "#Import pandas for data processing\n",
        "import pandas as pd\n",
        "\n",
        "# Make sure we can see all columns in this notebook\n",
        "pd.set_option(\"display.max_colwidth\", 999)\n",
        "pd.set_option(\"display.max_columns\", 999)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from this github. Ignore lines that can't be parsed\n",
        "df_complete = pd.read_csv(r\"https://raw.githubusercontent.com/thomasnamink/nlp-movie-recommender-case/main/movies.csv\", on_bad_lines=\"skip\")"
      ],
      "metadata": {
        "id": "tISqsa7hWQmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing/cleaning\n",
        "\n",
        "Since generating embeddings is quite slow, we first take a subset of the top 2500 most popular movies to do create our recommendations on."
      ],
      "metadata": {
        "id": "TJpRI-4sXibB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_complete.nlargest(2500, 'revenue').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "uIetZXZ7UKzI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: analyze the data, and create some descriptive statistics!\n",
        "df"
      ],
      "metadata": {
        "id": "wt8VOAqWe-_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's continue with doing some basic data cleaning of the movie descriptions"
      ],
      "metadata": {
        "id": "6TVyUkxIVBmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column, 'description_clean', and make sure this column is in string format\n",
        "df[\"description_clean\"] = df[\"description\"].astype(str)\n",
        "\n",
        "# Drop rows where the description isn't filled in correctly\n",
        "df = df[df[\"description_clean\"] != \"\"]\n",
        "df = df[df[\"description_clean\"].notnull()]"
      ],
      "metadata": {
        "id": "jja5KOIISXPF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning is an important step. Can you think of more data cleaning steps of the 'description_clean' column?"
      ],
      "metadata": {
        "id": "puH-2EhoVRGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: What cleaning steps would you like to do?\n",
        "..."
      ],
      "metadata": {
        "id": "lsywYBCFWwdI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome, you know have prepared the descriptions for using our NLP technique! Let's now pick a base movie you like (or dislike), to start generating recommendations for this movie, based on similar movie descriptions using NLP!"
      ],
      "metadata": {
        "id": "XnUOuFqCXmYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a movie for which you want to create recommendations\n",
        "df.head(50)['title'].tolist()"
      ],
      "metadata": {
        "id": "r2t3bRISV3e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "chosen_movie = \"Transformers\""
      ],
      "metadata": {
        "id": "3SuyoYgcWukG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Modelling\n",
        "\n",
        "Let's start our NLP modelling! Before we start, load in the model that we will use for creating the embeddings. As explained, this model can be used to generate 384 dimensional embeddings of the movie descriptions. More details about the specific model are explained below."
      ],
      "metadata": {
        "id": "WT68eZt8XNlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SentenceTransformer, to most easily calculate embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#Initialize the embedding model we have chosen for this case\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "vi7GrADAGd9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if our embedder correctly generates the embedding vector"
      ],
      "metadata": {
        "id": "pVKfcS97G41m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"Fill in something here\"\n",
        "model.encode(test_sentence)"
      ],
      "metadata": {
        "id": "R09d-tkCGqgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start generating the embeddings for all the movies! We do this similarly as in the example above, by adding a embedding column to our movie DataFrame. You can start by running the cell below.\n",
        "\n",
        "There are many many possible models you can use for calculating embeddings. The model we have chosen for this case is: sentence-transformers/all-MiniLM-L6-v2, a very popular model that can be used for calculating embeddings for texts in the English language. If you are curious about the embedding model, take a look at the HuggingFace page: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2."
      ],
      "metadata": {
        "id": "yZIfpGFwc-fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the tqdm package, that can easily be used for creating progress bars!\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Create an empty embedding column\n",
        "df[\"embedding\"] = None\n",
        "\n",
        "#Loop over all 2500 rows in our movie DataFrame, while using the tqdm for the progress bar. Encode the movie description at every row.\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    df.at[index, \"embedding\"] = model.encode(row[\"description_clean\"])\n",
        "\n",
        "df_with_embeddings = df.copy() #Save a copy so you won't lose the embedding if you accidentally do something wrong"
      ],
      "metadata": {
        "id": "QlpooMSCQ_OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some functions that will help us create the recommendations"
      ],
      "metadata": {
        "id": "QQEZNaQygsoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a,b):\n",
        "    \"\"\"\n",
        "    Get cosine similarity between two arrays a,b\n",
        "    :param a: array one\n",
        "    :param b: array two\n",
        "    \"\"\"\n",
        "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "    return cos_sim\n",
        "\n",
        "def get_closest(df, chosen_movie):\n",
        "    \"\"\"\n",
        "    Get most similar movie based on the index given\n",
        "    :param df: movies dataframe, including the embeddings\n",
        "    :param ix: Index of the movie you're interested in\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Finding closest movie for title: {chosen_movie}\")\n",
        "\n",
        "    #Try finding the base embedding\n",
        "    try:\n",
        "      base_embed = df[df['title'] == chosen_movie]['embedding'].iloc[0]\n",
        "    except:\n",
        "      raise Exception(\"Error finding base embedding! Did you specify a movie title that is in the movie dataset?\")\n",
        "\n",
        "    #Create an empty embedding column\n",
        "    df[\"similarity\"] = None\n",
        "\n",
        "    for i in df.index:\n",
        "        this_embed = df.at[i, 'embedding']\n",
        "        df.at[i, 'similarity'] = cosine_similarity(base_embed, this_embed)\n",
        "\n",
        "    return df[df[\"similarity\"].notnull()].sort_values(\"similarity\", ascending=False)"
      ],
      "metadata": {
        "id": "NAVl1OtPRHFO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can change your chosen movie here also\n",
        "#chosen_movie = \"Some other movie you want to try\"\n",
        "\n",
        "# Use get_closest to get movie that is most similar to your chosen movie\n",
        "df_results = get_closest(df, chosen_movie)\n",
        "\n",
        "#print the top 20 movies with the most similar embeddings\n",
        "df_results[[\"title\", \"description\", \"similarity\"]].head(10)"
      ],
      "metadata": {
        "id": "gZCzaaMMUPVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvements\n",
        "You've now found recommendations for your chosen movie, congratulations! What do you think about the quality of the recommendations?\n",
        "\n",
        "Right now, the movie recommendations are only based on the movie similarity, but more information about the movies is available! Can you think of a way to improve recommendations? Make sure your method for improving recommendations is general in the sense that it can be applied for any base movie that is chosen!"
      ],
      "metadata": {
        "id": "YuqpZAn9bbrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Right now, the recommendation score is only the similarity score\n",
        "df['score'] = df['similarity']"
      ],
      "metadata": {
        "id": "HoSxAii1sUuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Improve the recommendations\n",
        "df['score'] = df['similarity'] # + df['???']"
      ],
      "metadata": {
        "id": "X322ooffbiD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}